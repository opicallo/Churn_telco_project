#Librerías:
    
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from scipy.stats import norm, skew
from scipy import stats
import statsmodels.api as sm
import seaborn as sns
# sklearn modules for data preprocessing:
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from imblearn.over_sampling import SMOTE

#sklearn modules for Model Selection:
from sklearn import svm, tree, linear_model, neighbors
from sklearn import naive_bayes, ensemble, discriminant_analysis, gaussian_process
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
#sklearn modules for Model Evaluation & Improvement:
    
from sklearn.metrics import confusion_matrix, accuracy_score 
from sklearn.metrics import f1_score, precision_score, recall_score, fbeta_score
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import KFold
from sklearn import feature_selection
from sklearn import model_selection
from sklearn import metrics
from sklearn.metrics import classification_report, precision_recall_curve
from sklearn.metrics import auc, roc_auc_score, roc_curve
from sklearn.metrics import make_scorer, recall_score, log_loss
from sklearn.metrics import average_precision_score
#Standard libraries for data visualization:
import seaborn as sn
from matplotlib import pyplot
import matplotlib.pyplot as plt
import matplotlib.pylab as pylab
import matplotlib 
%matplotlib inline
color = sn.color_palette()
import matplotlib.ticker as mtick
from IPython.display import display
pd.options.display.max_columns = None
from pandas.plotting import scatter_matrix
from sklearn.metrics import roc_curve
#Miscellaneous Utilitiy Libraries:
    
import random
import os
import re
import sys
import timeit
import string
import time
from datetime import datetime
from time import time
from dateutil.parser import parse
import joblib


#Carga de datos y limpieza

dataset = pd.read_excel('Telco_customer_churn (3).xlsx')
dataset.head()
dataset.drop(dataset[['Churn Score', 'CLTV', 'Churn Reason']],axis=1, inplace=True)


dataset['Total Charges'] = pd.to_numeric(dataset['Total Charges'],errors='coerce')
dataset['Total Charges'] = dataset['Total Charges'].astype("float")

na_cols = dataset.isna().any()
na_cols = na_cols[na_cols == True].reset_index()
na_cols = na_cols["index"].tolist()
for col in dataset.columns[1:]:
     if col in na_cols:
        if dataset[col].dtype != 'object':
             dataset[col] =  dataset[col].fillna(dataset[col].mean()).round(0)
             
             
dataset.isna().any()
dataset.columns
dataset.describe()
dataset.dtypes
dataset.info()
dataset["Churn Value"].value_counts()

ax = sns.barplot(x="Churn Value", y="Zip Code", data=dataset, palette= 'BrBG_r',estimator=lambda x: len(x) / len(dataset) * 100)
ax.set(ylabel="Percent")





# Visualización Variables Categóricas Binarias (NO he hecho una función porque para comentarlas prefiero tenerlas individualmente) y su relación con el churn


ax = sns.barplot(x="Gender", y="Zip Code", data=dataset, palette= 'BrBG_r',estimator=lambda x: len(x) / len(dataset) * 100)
ax.set(ylabel="Percent")

dataset[["Gender","Churn Value"]].groupby('Gender').mean()

ax = sns.barplot(x="Senior Citizen", y="Zip Code", data=dataset, palette= 'BrBG_r',estimator=lambda x: len(x) / len(dataset) * 100)
ax.set(ylabel="Percent")

dataset[["Senior Citizen","Churn Value"]].groupby('Senior Citizen').mean()

ax = sns.barplot(x="Partner", y="Zip Code", data=dataset, palette= 'BrBG_r',estimator=lambda x: len(x) / len(dataset) * 100)
ax.set(ylabel="Percent")

dataset[["Partner","Churn Value"]].groupby('Partner').mean()

ax = sns.barplot(x="Dependents", y="Zip Code", data=dataset, palette= 'BrBG_r',estimator=lambda x: len(x) / len(dataset) * 100)
ax.set(ylabel="Percent")

dataset[["Dependents","Churn Value"]].groupby('Dependents').mean()

ax = sns.barplot(x="Phone Service", y="Zip Code", data=dataset, palette= 'BrBG_r',estimator=lambda x: len(x) / len(dataset) * 100)
ax.set(ylabel="Percent")

dataset[["Phone Service","Churn Value"]].groupby('Phone Service').mean()

ax = sns.barplot(x="Paperless Billing", y="Zip Code", data=dataset, palette= 'BrBG_r', estimator=lambda x: len(x) / len(dataset) * 100)
ax.set(ylabel="Percent")

dataset[["Paperless Billing","Churn Value"]].groupby('Paperless Billing').mean()





# Visualización Variables Categóricas con más de dos valores y su relación con el churn 


ax = sns.barplot(x="Streaming TV", y="Zip Code", data=dataset, palette= 'GnBu', estimator=lambda x: len(x) / len(dataset) * 100)
ax.set(ylabel="Percent")

dataset[["Streaming TV","Churn Value"]].groupby('Streaming TV').mean()

ax = sns.barplot(x="Streaming Movies", y="Zip Code", data=dataset, palette= 'GnBu', estimator=lambda x: len(x) / len(dataset) * 100)
ax.set(ylabel="Percent")

dataset[["Streaming Movies","Churn Value"]].groupby('Streaming Movies').mean()

ax = sns.barplot(x="Online Security", y="Zip Code", data=dataset, palette= 'GnBu', estimator=lambda x: len(x) / len(dataset) * 100)
ax.set(ylabel="Percent")

dataset[["Online Security","Churn Value"]].groupby('Online Security').mean()

ax = sns.barplot(x="Online Backup", y="Zip Code", data=dataset, palette= 'GnBu', estimator=lambda x: len(x) / len(dataset) * 100)
ax.set(ylabel="Percent")

dataset[["Online Backup","Churn Value"]].groupby('Online Backup').mean()

ax = sns.barplot(x="Device Protection", y="Zip Code", data=dataset, palette= 'GnBu', estimator=lambda x: len(x) / len(dataset) * 100)
ax.set(ylabel="Percent")

dataset[["Device Protection","Churn Value"]].groupby('Device Protection').mean()

ax = sns.barplot(x="Tech Support", y="Zip Code", data=dataset, palette= 'GnBu', estimator=lambda x: len(x) / len(dataset) * 100)
ax.set(ylabel="Percent")

dataset[["Tech Support","Churn Value"]].groupby('Tech Support').mean()

ax = sns.barplot(x="Multiple Lines", y="Zip Code", data=dataset, palette= 'GnBu', estimator=lambda x: len(x) / len(dataset) * 100)
ax.set(ylabel="Percent")

dataset[["Multiple Lines","Churn Value"]].groupby('Multiple Lines').mean()

ax = sns.barplot(x="Contract", y="Zip Code", data=dataset, palette= 'GnBu', estimator=lambda x: len(x) / len(dataset) * 100)
ax.set(ylabel="Percent")

dataset[["Contract","Churn Value"]].groupby('Contract').mean()

ax = sns.barplot(x="Payment Method", y="Zip Code", data=dataset, palette= 'GnBu', estimator=lambda x: len(x) / len(dataset) * 100)

dataset[["Payment Method","Churn Value"]].groupby('Payment Method').mean()




# Visualización Variables Continuas y su relación con el churn


ax=sns.distplot(dataset["Monthly Charges"], color= 'goldenrod')
ax.set(ylabel = "Churn Percentage")

dataset[["Monthly Charges","Churn Value"]].groupby('Churn Value').mean()


ax=sns.distplot(dataset["Tenure Months"], color= 'xkcd:sky blue')
ax.set(ylabel = "Churn Percentage")

dataset[["Tenure Months","Churn Value"]].groupby('Churn Value').mean()

ax=sns.distplot(dataset["Total Charges"], color= '#ab1239')
ax.set(ylabel = "Churn Percentage")

dataset[["Total Charges","Churn Value"]].groupby('Churn Value').mean()





# Preprocesamiento 


dataset.drop(['Zip Code','CustomerID', 'Count', 'State', 'Country', 'Lat Long', 'Latitude', 'Longitude','City','Churn Label'], axis=1, inplace=True)


Variables = ['Gender','Senior Citizen','Partner','Dependents','Phone Service','Paperless Billing','Multiple Lines', 'Internet Service', 'Online Security',
       'Online Backup', 'Device Protection', 'Tech Support', 'Streaming TV',
       'Streaming Movies', 'Payment Method', 'Contract']
X = pd.get_dummies(dataset, columns=Variables, drop_first=True)


labelencoder = LabelEncoder()

dataset['Gender'] = labelencoder.fit_transform(dataset['Gender'])
dataset['Senior Citizen'] = labelencoder.fit_transform(dataset['Senior Citizen'])
dataset['Partner'] = labelencoder.fit_transform(dataset['Partner'])
dataset['Dependents'] = labelencoder.fit_transform(dataset['Dependents'])
dataset['Phone Service'] = labelencoder.fit_transform(dataset['Phone Service'])
dataset['Paperless Billing'] = labelencoder.fit_transform(dataset['Paperless Billing'])



dataset3=dataset[['Gender','Senior Citizen', 'Partner','Dependents','Phone Service', 'Paperless Billing', 'Tenure Months', 'Monthly Charges','Total Charges']]

correlations = dataset3.corrwith(dataset['Churn Value'])

correlations.plot.bar(color= 'c', rot=45)

plt.ylabel("Churn Correlation")


data = dataset3.corr()

ax = sns.heatmap(
    data, 
    vmin=-1, vmax=1, center=0,
    cmap=sns.diverging_palette(20, 220, n=200),
    square=True
)
ax.set_xticklabels(
    ax.get_xticklabels(),
    rotation=45,
    horizontalalignment='right'
);


sc = MinMaxScaler()
X['Tenure Months']=sc.fit_transform(dataset[['Tenure Months']])
X['Monthly Charges']=sc.fit_transform(dataset[['Monthly Charges']])
X['Total Charges']=sc.fit_transform(dataset[['Total Charges']])



y = X['Churn Value']

X = X.drop(['Churn Value'], axis=1)


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1234)


sm = SMOTE(random_state=1234)

X_sm, y_sm = sm.fit_resample(X_train, y_train)

print(f'''Shape of X before SMOTE: {X.shape}
Shape of X after SMOTE: {X_sm.shape}''')

print('\nBalance of positive and negative classes (%):')
y_sm.value_counts(normalize=True) * 100






#Modelización



#LOGISTIC REGRESSION

log_model = LogisticRegression()
log_model.fit(X_sm, y_sm)
log_pred =  log_model.predict(X_test)

log_roc_auc_score_default = roc_auc_score(y_test, log_pred)
log_accuracy_default = accuracy_score(y_test, log_pred)

print("ROC AUC:",roc_auc_score(y_test, log_pred))
print("f1_score:",f1_score(y_test, log_pred))
print("accuracy:",metrics.accuracy_score(y_test, log_pred))
print("precision:",metrics.precision_score(y_test, log_pred))
print("recall:",metrics.recall_score(y_test, log_pred))

print(confusion_matrix(y_test, log_pred))
# classification report
print(classification_report(y_test, log_pred))

# confusion matrix
fig, ax = plt.subplots()
sns.heatmap(confusion_matrix(y_test, log_pred, normalize='true'), annot=True, ax=ax)
ax.set_title('Confusion Matrix')
ax.set_ylabel('Real Value')
ax.set_xlabel('Predicted Value')

plt.show()
                        

#RANDOM FOREST
RForest = RandomForestClassifier()
RForest.fit(X_sm, y_sm)
forest_pred= RForest.predict(X_test)

RForest_roc_auc_score_default = roc_auc_score(y_test, forest_pred)
RForesr_accuracy_default = accuracy_score(y_test, forest_pred)

print("ROC AUC:",roc_auc_score(y_test, forest_pred))
print("f1_score:",f1_score(y_test, forest_pred))
print("accuracy:",metrics.accuracy_score(y_test, forest_pred))
print("precision:",metrics.precision_score(y_test, forest_pred))
print("recall:",metrics.recall_score(y_test, forest_pred))

print(confusion_matrix(y_test, forest_pred))

# classification report
print(classification_report(y_test, forest_pred))

# confusion matrix
fig, ax = plt.subplots()
sns.heatmap(confusion_matrix(y_test, forest_pred, normalize='true'), annot=True, ax=ax)
ax.set_title('Confusion Matrix')
ax.set_ylabel('Real Value')
ax.set_xlabel('Predicted Value')

plt.show()




#SUPPORT VECTOR MACHINE


# Applying Support Vector Machine algorithm
from sklearn.svm import SVC  
svclassifier = SVC()  
svclassifier.fit(X_sm, y_sm)  
# Predicting part, applying the model to predict
y_pred = svclassifier.predict(X_test)  

print("ROC AUC:",roc_auc_score(y_test, y_pred))
print("f1_score:",f1_score(y_test, y_pred))
print("accuracy:",metrics.accuracy_score(y_test, y_pred))
print("precision:",metrics.precision_score(y_test, y_pred))
print("recall:",metrics.recall_score(y_test, y_pred))

print(confusion_matrix(y_test, y_pred))

# classification report
print(classification_report(y_test, y_pred))

# confusion matrix
fig, ax = plt.subplots()
sns.heatmap(confusion_matrix(y_test, y_pred, normalize='true'), annot=True, ax=ax)
ax.set_title('Confusion Matrix')
ax.set_ylabel('Real Value')
ax.set_xlabel('Predicted Value')

plt.show()




#XGBOOST


xgb_model = XGBClassifier()

xgb_model.fit(X_sm, y_sm)

y_pred = xgb_model.predict(X_test)
predictions = [round(value) for value in y_pred]

print("ROC AUC:",roc_auc_score(y_test, predictions))
print("f1_score:",f1_score(y_test, predictions))
print("accuracy:",metrics.accuracy_score(y_test, predictions))
print("precision:",metrics.precision_score(y_test, predictions))
print("recall:", metrics.recall_score(y_test, predictions))

print(confusion_matrix(y_test, predictions))

# classification report
print(classification_report(y_test, predictions))

# confusion matrix
fig, ax = plt.subplots()
sns.heatmap(confusion_matrix(y_test, predictions, normalize='true'), annot=True, ax=ax)
ax.set_title('Confusion Matrix')
ax.set_ylabel('Real Value')
ax.set_xlabel('Predicted Value')

plt.show()
